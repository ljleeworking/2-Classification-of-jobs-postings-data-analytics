{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Job Postings (Data Analytics)\n",
    "\n",
    "## Business Case Overview\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal has two main objectives:\n",
    "\n",
    "   1. Determine the industry factors that are most important in predicting the salary amounts for these data.\n",
    "   2. Determine the factors that distinguish job categories and titles from each other. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### QUESTION 1: Factors that impact salary\n",
    "\n",
    "To predict salary you can frame this as a classification problem, in which case you will create labels from these salaries (high vs. low salary, for example) according to thresholds (such as median salary).\n",
    "\n",
    "\n",
    "### QUESTION 2: Factors that distinguish job category\n",
    "\n",
    "There are a variety of interesting ways you can frame the target variable, for example:\n",
    "- What components of a job posting distinguish data scientists from other data jobs?\n",
    "- What features are important for distinguishing junior vs. senior positions?\n",
    "- Do the requirements for titles vary significantly with industry ?\n",
    "\n",
    "###  Overview:\n",
    "\n",
    "Part 1. Scrape and prepare your own data.\n",
    "\n",
    "Part 2. Data Cleaning and Exploratory data analysis (EDA)\n",
    "\n",
    "Part 3. Modelling and evaluation\n",
    "\n",
    "Part 4. Executive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 -  web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import csv\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping(job url links)\n",
    "# go to mycareersfuture website and search for \"data\"\n",
    "# search through the pages and get the url links to the jobs and save into csv file\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "links=[]\n",
    "\n",
    "# for loop to search through the pages and add the url links into the list\n",
    "for page in range(0, 2):  # the actual data was scraped using range(0,200)\n",
    "    sleep(random.randint(4,5))\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    driver.get(\"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page={}\".format(page))\n",
    "    \n",
    "    if len(soup)==0: \n",
    "        pass       # pass if no links is found\n",
    "    else:\n",
    "        for link in soup.find_all(\"a\", {\"class\": \"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\"}):\n",
    "            link = link.get(\"href\")\n",
    "            links.append(\"https://www.mycareersfuture.sg\"+link)\n",
    "\n",
    "# convert to dataframe\n",
    "links_df = pd.DataFrame({\"Links\":links})\n",
    "# save to csv\n",
    "links_df.to_csv(\"links_df_sample.csv\")\n",
    "# Closes the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. 1\n",
      "no. 2\n",
      "no. 3\n",
      "no. 4\n",
      "no. 5\n",
      "no. 6\n",
      "no. 7\n",
      "no. 8\n",
      "no. 9\n",
      "no. 10\n",
      "no. 11\n",
      "no. 12\n",
      "no. 13\n",
      "no. 14\n"
     ]
    }
   ],
   "source": [
    "# web scraping(job info)\n",
    "# load csv file with the url links\n",
    "csv = './links_df_sample.csv'  # actual file used is links_df.csv\n",
    "df = pd.read_csv(csv)\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "# Setting up lists\n",
    "company=[]\n",
    "job_title=[]\n",
    "location=[]\n",
    "employment_type=[]\n",
    "seniority=[]\n",
    "job_categories=[]\n",
    "salary=[]\n",
    "payment_period=[]\n",
    "job_description=[]\n",
    "requirements=[]\n",
    "count=0\n",
    "\n",
    "# search through links to get the job info, append Nan if info not found\n",
    "\n",
    "for info in df.Links:\n",
    "    sleep(random.randint(8,10))\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    driver.get(info)\n",
    "    count +=1\n",
    "    sleep(random.randint(5,8))\n",
    "    print('no.',count)\n",
    "    \n",
    "    try:\n",
    "        company.append(driver.find_element_by_name('company').text)\n",
    "    except:\n",
    "        company.append(np.nan)\n",
    "    try:\n",
    "        job_title.append(driver.find_element_by_id('job_title').text)\n",
    "    except:\n",
    "        job_title.append(np.nan)    \n",
    "    try:\n",
    "        location.append(driver.find_element_by_id('address').text)\n",
    "    except:\n",
    "        location.append(np.nan)  \n",
    "    try:\n",
    "        employment_type.append(driver.find_element_by_id('employment_type').text)\n",
    "    except:\n",
    "        employment_type.append(np.nan)\n",
    "    try:\n",
    "        seniority.append(driver.find_element_by_id('seniority').text)\n",
    "    except:\n",
    "        seniority.append(np.nan)\n",
    "    try:\n",
    "        job_categories.append(driver.find_element_by_id('job-categories').text)\n",
    "    except:\n",
    "        job_categories.append(np.nan)\n",
    "    try:\n",
    "        salary.append(driver.find_element_by_class_name('lh-solid').text)\n",
    "    except:\n",
    "        salary.append(np.nan)\n",
    "    try:\n",
    "        payment_period.append(driver.find_element_by_class_name('salary_type').text)\n",
    "    except:\n",
    "        payment_period.append(np.nan)      \n",
    "    try:\n",
    "        job_description.append(driver.find_element_by_id('job_description').text)\n",
    "    except:\n",
    "        job_description.append(np.nan)\n",
    "    try:\n",
    "        requirements.append(driver.find_element_by_id('requirements').text)\n",
    "    except:\n",
    "        requirements.append(np.nan)\n",
    "\n",
    "# save info into a dataframe\n",
    "jobs_info = pd.DataFrame({'company':company,'job_title':job_title,'location':location,'employment_type':employment_type,'seniority':seniority,'job_categories':job_categories,'salary':salary,'payment_period':payment_period,'job_description':job_description, 'requirements':requirements })\n",
    "\n",
    "# add to the first links dataframe\n",
    "result = pd.concat([df,jobs_info], axis=1,sort=False)\n",
    "# save to a new csv file\n",
    "result.to_csv('job_info_sample.csv') # actual file used is job_info.csv\n",
    "\n",
    "# Closes the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
